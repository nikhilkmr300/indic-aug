from collections import OrderedDict
import shutil
import os
import sys

import numpy as np
import pandas as pd
import sentencepiece as spm

from .globals import ERRORS, LANGS
from .globals import PAD_TOKEN, PAD_ID, UNK_TOKEN, UNK_ID, SOS_TOKEN, SOS_ID, EOS_TOKEN, EOS_ID, BLANK_TOKEN
from .utils import path2lang

class Vocab:
    """Utility class for vocabulary-related functions.

    ``vocab_dir`` is a directory containing \*.model and \*.vocab files as
    generated by ``sentencepiece`` or ``Vocab.build``. The files in this
    directory must be named as ``<lang_code>.model`` and ``<lang_code>.vocab``
    (automatically done if you use ``Vocab.build``), where ``lang_code`` is the
    ISO 639-1 code for that language. For example, if you were translating
    between English and Hindi, the directory would be structured as:

    <vocab_dir>
    ├── en.model
    ├── en.vocab
    ├── hi.model
    └── hi.vocab
    """

    @classmethod
    def load_model(cls, vocab_dir, lang):
        """Returns contents of ``sentencepiece`` \*.model file.

        :param vocab_dir: As described in the docstring for this class.
        :type vocab_dir: str
        :param lang: ISO 631-9 code for language.
        :type lang: str

        :return: ``sentencepiece`` model.
        :rtype: ``sentencepiece.SentencePieceProcessor``
        """

        model_path = os.path.join(vocab_dir, f'{lang}.model')
        model = spm.SentencePieceProcessor()
        model.load(model_path)

        return model

    @classmethod
    def load_vocab(cls, vocab_dir, lang):
        """Returns vocabulary in ``sentencepiece`` \*.vocab file as a list.

        :param vocab_dir: As described in the docstring for this class.
        :type vocab_dir: str
        :param lang: ISO 631-9 code for language.
        :type lang: str

        :return: List of words in vocabulary.
        :rtype: list
        """

        vocab_path = os.path.join(vocab_dir, f'{lang}.vocab')

        vocab = pd.read_csv(vocab_path, sep='\t', header=None)
        vocab = vocab[0].str.strip('▁').squeeze().tolist()      # Note: '▁' is NOT the same as underscore ('_').

        return vocab

    @classmethod
    def build(cls, src_path, tgt_path, src_vocab_size, tgt_vocab_size, vocab_dir):
        """Trains a ``sentencepiece`` model and generates the vocabulary.

        :param src_path: Path to source portion of parallel corpus.
        :type src_path: str
        :param tgt_path: Path to target portion of parallel corpus.
        :type tgt_path: str
        :param src_vocab_size: Number of tokens in source vocabulary.
        :type src_vocab_size: int
        :param tgt_vocab_size: Number of tokens in targert vocabulary.
        :type tgt_vocab_size: int
        :param vocab_dir: As described in the docstring for this class.
        :type vocab_dir: str
        """

        src_lang = path2lang(src_path)
        tgt_lang = path2lang(tgt_path)

        loglevel = 3
        if src_vocab_size == -1:
            # Using all words as vocabulary.
            spm.SentencePieceTrainer.train(f'--input={src_path} --model_prefix={src_lang} --model_type=word --use_all_vocab=true --normalization_rule_name=nmt_nfkc --pad_id={PAD_ID} --pad_piece={PAD_TOKEN} --unk_id={UNK_ID} --unk_piece={UNK_TOKEN} --bos_id={SOS_ID} --bos_piece={SOS_TOKEN} --eos_id={EOS_ID} --eos_piece={EOS_TOKEN} --control_symbols={BLANK_TOKEN} --minloglevel={loglevel}')
        else:
            spm.SentencePieceTrainer.train(f'--input={src_path} --model_prefix={src_lang} --model_type=word --vocab_size={src_vocab_size} --normalization_rule_name=nmt_nfkc --pad_id={PAD_ID} --pad_piece={PAD_TOKEN} --unk_id={UNK_ID} --unk_piece={UNK_TOKEN} --bos_id={SOS_ID} --bos_piece={SOS_TOKEN} --eos_id={EOS_ID} --eos_piece={EOS_TOKEN} --control_symbols={BLANK_TOKEN} --minloglevel={loglevel}')

        if tgt_vocab_size == -1:
            # Using all words as vocabulary.
            spm.SentencePieceTrainer.train(f'--input={tgt_path} --model_prefix={tgt_lang} --model_type=word --use_all_vocab=true --normalization_rule_name=nmt_nfkc --pad_id={PAD_ID} --pad_piece={PAD_TOKEN} --unk_id={UNK_ID} --unk_piece={UNK_TOKEN} --bos_id={SOS_ID} --bos_piece={SOS_TOKEN} --eos_id={EOS_ID} --eos_piece={EOS_TOKEN} --control_symbols={BLANK_TOKEN} --minloglevel={loglevel}')
        else:
            spm.SentencePieceTrainer.train(f'--input={tgt_path} --model_prefix={tgt_lang} --model_type=word --vocab_size={tgt_vocab_size} --normalization_rule_name=nmt_nfkc --pad_id={PAD_ID} --pad_piece={PAD_TOKEN} --unk_id={UNK_ID} --unk_piece={UNK_TOKEN} --bos_id={SOS_ID} --bos_piece={SOS_TOKEN} --eos_id={EOS_ID} --eos_piece={EOS_TOKEN} --control_symbols={BLANK_TOKEN} --minloglevel={loglevel}')

        if os.path.exists(vocab_dir):
            shutil.rmtree(vocab_dir)
        os.makedirs(vocab_dir)

        shutil.move(f'{src_lang}.model', os.path.join(vocab_dir, f'{src_lang}.model'))
        shutil.move(f'{src_lang}.vocab', os.path.join(vocab_dir, f'{src_lang}.vocab'))
        shutil.move(f'{tgt_lang}.model', os.path.join(vocab_dir, f'{tgt_lang}.model'))
        shutil.move(f'{tgt_lang}.vocab', os.path.join(vocab_dir, f'{tgt_lang}.vocab'))

    @classmethod
    def score2freq(cls, vocab_dir, lang):
        """Returns frequencies of all words in the vocabulary, reverse
        calculated from ``sentencepiece`` score (negative log likelihood).

        :param vocab_dir: As described in docstring for this class.
        :type vocab_dir: str
        :param lang: ISO 631-9 code for language.
        :type lang: str

        :return: Map of word in vocabulary to its frequency.
        :rtype: dict
        """

        model = Vocab.load_model(vocab_dir, lang)
        vocab = Vocab.load_vocab(vocab_dir, lang)

        freq_dict = dict()
        word_ids = [model.encode(word)[0] for word in vocab]
        for word_id in word_ids:
            word = model.IdToPiece(word_id).strip('▁')
            score = model.GetScore(word_id)
            if not score:
                # sentencepiece maps unseen tokens (<unk>, <s>, </s>) to 0 ==> freq = 1, when their freq should be 0.
                freq_dict[word] = 0
            else:
                # sentencepiece outputs negative log likelihood as score. Taking exponent to convert it to frequency.
                freq_dict[word] = np.exp(score)

        # If you are using all the words as vocabulary, this should sum to 1.
        assert sum(freq_dict.values()) <= 1

        return freq_dict

    @classmethod
    def read_topn(cls, vocab_dir, lang, n):
        """Reads the ``n`` most frequent tokens in vocabulary into a list.

        :param vocab_dir: As described in docstring for this class.
        :type vocab_dir: str
        :param lang: ISO 631-9 code for language.
        :type lang: str
        :param n: Number of words.
        :type n: int

        :return: ``n`` most frequent tokens
        :rtype: list
        """

        freq_dict = Vocab.score2freq(vocab_dir, lang)

        return OrderedDict(sorted(freq_dict.items(), key=lambda x: x[1])[-n:])

    @classmethod
    def read_bottomn(cls, vocab_dir, lang, n):
        """Reads the ``n`` least frequent tokens in vocabulary into a list.

        :param vocab_dir: As described in docstring for this class.
        :type vocab_dir: str
        :param lang: ISO 631-9 code for language.
        :type lang: str
        :param n: Number of words.
        :type n: int

        :return: ``n`` least frequent tokens
        :rtype: list
        """

        freq_dict = Vocab.score2freq(vocab_dir, lang)

        return OrderedDict(sorted(freq_dict.items(), key=lambda x: x[1])[:n])