@misc{duan2020syntax,
  title={Syntax-aware Data Augmentation for Neural Machine Translation}, 
  author={Sufeng Duan and Hai Zhao and Dongdong Zhang and Rui Wang},
  year={2020},
  eprint={2004.14200},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{xie2017data,
  author    = {Ziang Xie and
               Sida I. Wang and
               Jiwei Li and
               Daniel L{\'{e}}vy and
               Aiming Nie and
               Dan Jurafsky and
               Andrew Y. Ng},
  title     = {Data Noising as Smoothing in Neural Network Language Models},
  journal   = {CoRR},
  volume    = {abs/1703.02573},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.02573},
  archivePrefix = {arXiv},
  eprint    = {1703.02573},
  timestamp = {Mon, 13 Aug 2018 16:47:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/XieWLLNJN17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{iyyer2015deep,
    title = "Deep Unordered Composition Rivals Syntactic Methods for Text Classification",
    author = "Iyyer, Mohit  and
      Manjunatha, Varun  and
      Boyd-Graber, Jordan  and
      Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1162",
    doi = "10.3115/v1/P15-1162",
    pages = "1681--1691",
}

@article{wei2019eda,
  author    = {Jason W. Wei and
               Kai Zou},
  title     = {{EDA:} Easy Data Augmentation Techniques for Boosting Performance
               on Text Classification Tasks},
  journal   = {CoRR},
  volume    = {abs/1901.11196},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.11196},
  archivePrefix = {arXiv},
  eprint    = {1901.11196},
  timestamp = {Mon, 04 Feb 2019 08:11:03 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-11196.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fadaee2017data,
  author    = {Marzieh Fadaee and
               Arianna Bisazza and
               Christof Monz},
  title     = {Data Augmentation for Low-Resource Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1705.00440},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.00440},
  archivePrefix = {arXiv},
  eprint    = {1705.00440},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FadaeeBM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{arora2020inltk,
    title = "i{NLTK}: Natural Language Toolkit for Indic Languages",
    author = "Arora, Gaurav",
    booktitle = "Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlposs-1.10",
    doi = "10.18653/v1/2020.nlposs-1.10",
    pages = "66--71",
    abstract = "We present iNLTK, an open-source NLP library consisting of pre-trained language models and out-of-the-box support for Data Augmentation, Textual Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text Generation in 13 Indic Languages. By using pre-trained models from iNLTK for text classification on publicly available datasets, we significantly outperform previously reported results. On these datasets, we also show that by using pre-trained models and data augmentation from iNLTK, we can achieve more than 95{\%} of the previous best performance by using less than 10{\%} of the training data. iNLTK is already being widely used by the community and has 40,000+ downloads, 600+ stars and 100+ forks on GitHub. The library is available at https://github.com/goru001/inltk.",
}