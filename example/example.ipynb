{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483ffd63-514f-4760-ac5e-6bdb0baae582",
   "metadata": {},
   "source": [
    "# How to Use This Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d2c24-ca77-4d18-ac41-6858abd4e1bd",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d85ac3-e810-42b7-a257-83b2273df227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59da6b-7dbc-426e-850c-4cd1cf569799",
   "metadata": {},
   "source": [
    "## Getting the data for this example\n",
    "\n",
    "This example uses the data available from [here](http://www.manythings.org/anki/). Steps to obtain the data (run the following cells):\n",
    "1. Download the Hindi-English parallel corpus (`hin-eng.zip`) and save it in the `raw` directory. \n",
    "2. Switch to the `raw` directory using `cd raw` and unzip `hin-eng.zip`.\n",
    "3. Run the script `prepare_data.sh` using `./prepare_data.sh`.\n",
    "\n",
    "When using your own data, make sure the bilingual corpus is split into two files, one for each language. The files must have the extension as the [ISO 639-1 code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) for the language. For example, the file containing English sentences in this example is named `data.en` and the file containing the corresponding Hindi sentences is named `data.hi`. Make sure that the document on line number `n` in the target file (`data.hi` in this case) is a translation of the corresponding document on line number `n` in the source file (`data.en` in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e22f5e-9a1f-4647-9242-8753b328154c",
   "metadata": {},
   "source": [
    "Looking at the raw data. It is neither normalized nor tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4eccbc-f54a-431c-a5c7-7100deb9b654",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                      0  \\\n",
       "2910  If you go to that supermarket, you can buy mos...   \n",
       "2911  The passengers who were injured in the acciden...   \n",
       "2912  Democracy is the worst form of government, exc...   \n",
       "2913  If my boy had not been killed in the traffic a...   \n",
       "2914  When I was a kid, touching bugs didn't bother ...   \n",
       "\n",
       "                                                      1  \n",
       "2910  उस सूपरमार्केट में तुम लगभग कोई भी रोजाने में ...  \n",
       "2911  जिन यात्रियों को दुर्घटना मे चोट आई थी उन्हे अ...  \n",
       "2912  लोकतंत्र सरकार का सबसे घिनौना रूप है, अगर बाकी...  \n",
       "2913  अगर मेरा बेटा ट्रेफ़िक हादसे में नहीं मारा गया...  \n",
       "2914  जब मैं बच्चा था, मुझे कीड़ों को छूने से कोई पर...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2910</th>\n      <td>If you go to that supermarket, you can buy mos...</td>\n      <td>उस सूपरमार्केट में तुम लगभग कोई भी रोजाने में ...</td>\n    </tr>\n    <tr>\n      <th>2911</th>\n      <td>The passengers who were injured in the acciden...</td>\n      <td>जिन यात्रियों को दुर्घटना मे चोट आई थी उन्हे अ...</td>\n    </tr>\n    <tr>\n      <th>2912</th>\n      <td>Democracy is the worst form of government, exc...</td>\n      <td>लोकतंत्र सरकार का सबसे घिनौना रूप है, अगर बाकी...</td>\n    </tr>\n    <tr>\n      <th>2913</th>\n      <td>If my boy had not been killed in the traffic a...</td>\n      <td>अगर मेरा बेटा ट्रेफ़िक हादसे में नहीं मारा गया...</td>\n    </tr>\n    <tr>\n      <th>2914</th>\n      <td>When I was a kid, touching bugs didn't bother ...</td>\n      <td>जब मैं बच्चा था, मुझे कीड़ों को छूने से कोई पर...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_df = pd.read_csv(os.path.join('raw', 'hin.txt'), sep='\\t', header=None)\n",
    "raw_df.iloc[:, 0:2].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2f3f5-0ce3-4add-937f-0660ac719bcf",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The `preprocess` module has two builtin preprocessing functions, one for normalizing the textual data and another for tokenizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3691aff9-ed92-4029-8ebb-93592168096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indic_aug.preprocess import Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93451614-4477-4d98-8ce9-b3567eda3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf preprocessed\n",
    "mkdir preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5d433b-36c4-4448-9f21-bedc7ed4916f",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s][batch_size=64, batch_count=46, funcs=('normalize', 'pretokenize')]\n",
      "100%|██████████| 46/46 [00:07<00:00,  6.34it/s]\n"
     ]
    }
   ],
   "source": [
    "preproc = Preprocess(\n",
    "    os.path.join('raw', 'data.en'),\n",
    "    os.path.join('raw', 'data.hi')\n",
    ")\n",
    "preproc.preprocess(\n",
    "    os.path.join('preprocessed', 'data.en'),\n",
    "    os.path.join('preprocessed', 'data.hi'),\n",
    "    64,                                         # Number of parallel documents to bring into memory at a time.\n",
    "    funcs=['normalize', 'pretokenize']          # Functions applied in order specified.\n",
    ")"
   ]
  },
  {
   "source": [
    "## Building the vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf vocab\n",
    "mkdir vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indic_aug.vocab import build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_vocab(\n",
    "    os.path.join('preprocessed', 'data.en'),\n",
    "    os.path.join('preprocessed', 'data.hi'),\n",
    "    2000,           # Source (en) vocab size\n",
    "    2000,           # Target (hi) vocab size\n",
    "    'vocab'\n",
    ")"
   ]
  },
  {
   "source": [
    "## SynonymAugmentor\n",
    "\n",
    "This section shows how to use the `SynonymAugmentor` from the `basic` package. Other augmentors from the `basic` package can be used similarly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indic_aug.basic import SynonymAugmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-04-28 02:34:49 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-28:02:34:49,885 INFO     [core.py:104] Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-28 02:34:49 INFO: Use device: cpu\n",
      "2021-04-28:02:34:49,895 INFO     [core.py:115] Use device: cpu\n",
      "2021-04-28 02:34:49 INFO: Loading: tokenize\n",
      "2021-04-28:02:34:49,932 INFO     [core.py:121] Loading: tokenize\n",
      "2021-04-28 02:34:49 INFO: Loading: pos\n",
      "2021-04-28:02:34:49,969 INFO     [core.py:121] Loading: pos\n",
      "2021-04-28 02:34:50 INFO: Loading: lemma\n",
      "2021-04-28:02:34:50,769 INFO     [core.py:121] Loading: lemma\n",
      "2021-04-28 02:34:51 INFO: Loading: depparse\n",
      "2021-04-28:02:34:51,363 INFO     [core.py:121] Loading: depparse\n",
      "2021-04-28 02:34:52 INFO: Loading: sentiment\n",
      "2021-04-28:02:34:52,164 INFO     [core.py:121] Loading: sentiment\n",
      "2021-04-28 02:34:52 INFO: Loading: ner\n",
      "2021-04-28:02:34:52,929 INFO     [core.py:121] Loading: ner\n",
      "2021-04-28 02:34:53 INFO: Done loading processors!\n",
      "2021-04-28:02:34:53,756 INFO     [core.py:166] Done loading processors!\n",
      "2021-04-28 02:34:53 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2021-04-28:02:34:53,764 INFO     [core.py:104] Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2021-04-28 02:34:53 INFO: Use device: cpu\n",
      "2021-04-28:02:34:53,770 INFO     [core.py:115] Use device: cpu\n",
      "2021-04-28 02:34:53 INFO: Loading: tokenize\n",
      "2021-04-28:02:34:53,788 INFO     [core.py:121] Loading: tokenize\n",
      "2021-04-28 02:34:53 INFO: Loading: pos\n",
      "2021-04-28:02:34:53,828 INFO     [core.py:121] Loading: pos\n",
      "2021-04-28 02:34:54 INFO: Loading: lemma\n",
      "2021-04-28:02:34:54,278 INFO     [core.py:121] Loading: lemma\n",
      "2021-04-28 02:34:54 INFO: Loading: depparse\n",
      "2021-04-28:02:34:54,356 INFO     [core.py:121] Loading: depparse\n",
      "2021-04-28 02:34:55 INFO: Done loading processors!\n",
      "2021-04-28:02:34:55,98 INFO     [core.py:166] Done loading processors!\n",
      "2021-04-28:02:34:55,101 INFO     [iwn.py:43] Loading hindi language synsets...\n"
     ]
    }
   ],
   "source": [
    "synonymAugmentor = SynonymAugmentor(\n",
    "    os.path.join('preprocessed', 'data.en'),\n",
    "    os.path.join('preprocessed', 'data.hi'),\n",
    "    0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Wow', 'वाह')\n",
      "('Help', 'बचाओ')\n",
      "('Jump', 'उछलो')\n",
      "('Jump', 'कूदो')\n",
      "('Jump', 'छलांग')\n",
      "('Hello', 'नमस्ते')\n",
      "('Hello', 'आदेस')\n",
      "('Cheers', 'वाह - वाह')\n",
      "('Cheers', 'चियर्स')\n",
      "('suffer it', 'समझे कि नहीं')\n",
      "('Im OK', 'मैं ठीक हूं')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for augmentedPair in synonymAugmentor:\n",
    "    print(augmentedPair)\n",
    "    if i == 10:\n",
    "        break\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "source": [
    "## DepParseAugmentor\n",
    "\n",
    "This section shows how to use `DepParseTree` and `DepParseAugmentor` from the `depparse` package."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import graphviz\n",
    "\n",
    "from indic_aug.depparse import DepParseTree, DepParseAugmentor"
   ]
  },
  {
   "source": [
    "Creating the sentence in the desired form as expected by `DepParseTree`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-04-28 02:35:02 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-28:02:35:02,268 INFO     [core.py:104] Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-28 02:35:02 INFO: Use device: cpu\n",
      "2021-04-28:02:35:02,280 INFO     [core.py:115] Use device: cpu\n",
      "2021-04-28 02:35:02 INFO: Loading: tokenize\n",
      "2021-04-28:02:35:02,290 INFO     [core.py:121] Loading: tokenize\n",
      "2021-04-28 02:35:02 INFO: Loading: pos\n",
      "2021-04-28:02:35:02,314 INFO     [core.py:121] Loading: pos\n",
      "2021-04-28 02:35:02 INFO: Loading: lemma\n",
      "2021-04-28:02:35:02,981 INFO     [core.py:121] Loading: lemma\n",
      "2021-04-28 02:35:03 INFO: Loading: depparse\n",
      "2021-04-28:02:35:03,70 INFO     [core.py:121] Loading: depparse\n",
      "2021-04-28 02:35:03 INFO: Loading: sentiment\n",
      "2021-04-28:02:35:03,727 INFO     [core.py:121] Loading: sentiment\n",
      "2021-04-28 02:35:04 INFO: Loading: ner\n",
      "2021-04-28:02:35:04,665 INFO     [core.py:121] Loading: ner\n",
      "2021-04-28 02:35:06 INFO: Done loading processors!\n",
      "2021-04-28:02:35:06,143 INFO     [core.py:166] Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline()\n",
    "sent = nlp('Jack hit the ball with the bat').sentences[0]"
   ]
  },
  {
   "source": [
    "Creating the dependency parse tree and saving it as a GraphViz DOT file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptree = DepParseTree(sent)\n",
    "deptree.save_tree('deptree.dot')"
   ]
  },
  {
   "source": [
    "Plotting the graph."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<graphviz.files.Source at 0x7fa46892d6d8>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"318pt\" height=\"260pt\"\n viewBox=\"0.00 0.00 318.00 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 314,-256 314,4 -4,4\"/>\n<!-- 2 -->\n<g id=\"node1\" class=\"node\">\n<title>2</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"148,-180 89,-180 89,-144 148,-144 148,-180\"/>\n<text text-anchor=\"middle\" x=\"118.5\" y=\"-158.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">2:hit</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"67,-108 0,-108 0,-72 67,-72 67,-108\"/>\n<text text-anchor=\"middle\" x=\"33.5\" y=\"-86.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">1:Jack</text>\n</g>\n<!-- 2&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>2&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M97.49,-143.7C86.78,-134.88 73.61,-124.03 62,-114.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"64.21,-111.76 54.27,-108.1 59.76,-117.16 64.21,-111.76\"/>\n</g>\n<!-- 4 -->\n<g id=\"node4\" class=\"node\">\n<title>4</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"152,-108 85,-108 85,-72 152,-72 152,-108\"/>\n<text text-anchor=\"middle\" x=\"118.5\" y=\"-86.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">4:ball</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M118.5,-143.7C118.5,-135.98 118.5,-126.71 118.5,-118.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"122,-118.1 118.5,-108.1 115,-118.1 122,-118.1\"/>\n</g>\n<!-- 7 -->\n<g id=\"node6\" class=\"node\">\n<title>7</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"229,-108 170,-108 170,-72 229,-72 229,-108\"/>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-86.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">7:bat</text>\n</g>\n<!-- 2&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>2&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M138.52,-143.7C148.63,-134.97 161.04,-124.24 172.02,-114.75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"174.43,-117.29 179.71,-108.1 169.85,-111.99 174.43,-117.29\"/>\n</g>\n<!-- 0 -->\n<g id=\"node3\" class=\"node\">\n<title>0</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"160.5,-252 76.5,-252 76.5,-216 160.5,-216 160.5,-252\"/>\n<text text-anchor=\"middle\" x=\"118.5\" y=\"-230.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">0:&lt;root&gt;</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M118.5,-215.7C118.5,-207.98 118.5,-198.71 118.5,-190.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"122,-190.1 118.5,-180.1 115,-190.1 122,-190.1\"/>\n</g>\n<!-- 3 -->\n<g id=\"node5\" class=\"node\">\n<title>3</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"148,-36 89,-36 89,0 148,0 148,-36\"/>\n<text text-anchor=\"middle\" x=\"118.5\" y=\"-14.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">3:the</text>\n</g>\n<!-- 4&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M118.5,-71.7C118.5,-63.98 118.5,-54.71 118.5,-46.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"122,-46.1 118.5,-36.1 115,-46.1 122,-46.1\"/>\n</g>\n<!-- 5 -->\n<g id=\"node7\" class=\"node\">\n<title>5</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"233,-36 166,-36 166,0 233,0 233,-36\"/>\n<text text-anchor=\"middle\" x=\"199.5\" y=\"-14.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">5:with</text>\n</g>\n<!-- 7&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>7&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M199.5,-71.7C199.5,-63.98 199.5,-54.71 199.5,-46.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203,-46.1 199.5,-36.1 196,-46.1 203,-46.1\"/>\n</g>\n<!-- 6 -->\n<g id=\"node8\" class=\"node\">\n<title>6</title>\n<polygon fill=\"mediumorchid\" stroke=\"black\" points=\"310,-36 251,-36 251,0 310,0 310,-36\"/>\n<text text-anchor=\"middle\" x=\"280.5\" y=\"-14.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">6:the</text>\n</g>\n<!-- 7&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>7&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M219.52,-71.7C229.63,-62.97 242.04,-52.24 253.02,-42.75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"255.43,-45.29 260.71,-36.1 250.85,-39.99 255.43,-45.29\"/>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "graphviz.Source.from_file('deptree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-04-28 02:35:08 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-28:02:35:08,877 INFO     [core.py:104] Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-28 02:35:08 INFO: Use device: cpu\n",
      "2021-04-28:02:35:08,888 INFO     [core.py:115] Use device: cpu\n",
      "2021-04-28 02:35:08 INFO: Loading: tokenize\n",
      "2021-04-28:02:35:08,896 INFO     [core.py:121] Loading: tokenize\n",
      "2021-04-28 02:35:08 INFO: Loading: pos\n",
      "2021-04-28:02:35:08,935 INFO     [core.py:121] Loading: pos\n",
      "2021-04-28 02:35:09 INFO: Loading: lemma\n",
      "2021-04-28:02:35:09,537 INFO     [core.py:121] Loading: lemma\n",
      "2021-04-28 02:35:09 INFO: Loading: depparse\n",
      "2021-04-28:02:35:09,607 INFO     [core.py:121] Loading: depparse\n",
      "2021-04-28 02:35:11 INFO: Loading: sentiment\n",
      "2021-04-28:02:35:11,399 INFO     [core.py:121] Loading: sentiment\n",
      "2021-04-28 02:35:12 INFO: Loading: ner\n",
      "2021-04-28:02:35:12,572 INFO     [core.py:121] Loading: ner\n",
      "2021-04-28 02:35:14 INFO: Done loading processors!\n",
      "2021-04-28:02:35:14,556 INFO     [core.py:166] Done loading processors!\n",
      "2021-04-28 02:35:14 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2021-04-28:02:35:14,581 INFO     [core.py:104] Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2021-04-28 02:35:14 INFO: Use device: cpu\n",
      "2021-04-28:02:35:14,620 INFO     [core.py:115] Use device: cpu\n",
      "2021-04-28 02:35:14 INFO: Loading: tokenize\n",
      "2021-04-28:02:35:14,660 INFO     [core.py:121] Loading: tokenize\n",
      "2021-04-28 02:35:14 INFO: Loading: pos\n",
      "2021-04-28:02:35:14,777 INFO     [core.py:121] Loading: pos\n",
      "2021-04-28 02:35:15 INFO: Loading: lemma\n",
      "2021-04-28:02:35:15,990 INFO     [core.py:121] Loading: lemma\n",
      "2021-04-28 02:35:16 INFO: Loading: depparse\n",
      "2021-04-28:02:35:16,97 INFO     [core.py:121] Loading: depparse\n",
      "2021-04-28 02:35:17 INFO: Done loading processors!\n",
      "2021-04-28:02:35:17,777 INFO     [core.py:166] Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "depparseAugmentor = DepParseAugmentor(\n",
    "    os.path.join('preprocessed', 'data.en'),\n",
    "    os.path.join('preprocessed', 'data.hi'),\n",
    "    'blank',\n",
    "    0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Wow !', 'वाह !')\n",
      "('Help !', 'बचाओ !')\n",
      "('Jump .', 'उछलो ।')\n",
      "('Jump <blank>', 'कूदो ।')\n",
      "('Jump .', 'छलांग ।')\n",
      "('Hello <blank>', 'नमस्ते ।')\n",
      "('<blank> <blank>', 'नमस्कार ।')\n",
      "('Cheers <blank>', 'वाह - <blank> !')\n",
      "('Cheers !', 'चियर्स <blank>')\n",
      "('Got it <blank>', 'समझे कि नहीं ?')\n",
      "('Im <blank> .', 'मैं ठीक हूं ।')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for augmentedPair in depparseAugmentor:\n",
    "    print(augmentedPair)\n",
    "    if i == 100:\n",
    "        break\n",
    "    else:\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python370jvsc74a57bd0be06934d709f3841506e224c246d1fa2d8b3c14deee322268a97364bbcca65ac",
   "display_name": "Python 3.7.0 64-bit ('nmt': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}